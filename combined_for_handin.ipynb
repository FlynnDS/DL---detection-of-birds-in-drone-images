{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6336f32ec32618c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T12:11:43.461148Z",
     "start_time": "2025-05-12T12:11:41.355221Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from itertools import product\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image as PILImage\n",
    "from pydantic import BaseModel\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ace8e1b11c658e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T12:11:43.482488Z",
     "start_time": "2025-05-12T12:11:43.477790Z"
    }
   },
   "outputs": [],
   "source": [
    "# datasets\n",
    "folder_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# models\n",
    "models = os.path.join(os.getcwd(), 'models')\n",
    "\n",
    "# logs\n",
    "logs = os.path.join(os.getcwd(), 'logs')\n",
    "\n",
    "yolo_data = os.path.join(folder_path, 'tiled_data', 'data.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb845ab568d51f",
   "metadata": {},
   "source": [
    "# Gamma correction code\n",
    "These function will get executed during the image slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762bc6f360115b8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:14:10.972133Z",
     "start_time": "2025-05-12T11:14:10.966051Z"
    }
   },
   "outputs": [],
   "source": [
    "def gamma_correction(image, gamma=1.0):\n",
    "    # Ensure gamma is a positive number\n",
    "    if gamma <= 0:\n",
    "        raise ValueError(\"Gamma should be greater than 0\")\n",
    "\n",
    "    # Build a lookup table mapping pixel values [0, 255] to their gamma-corrected values\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([\n",
    "        ((i / 255.0) ** inv_gamma) * 255\n",
    "        for i in range(256)\n",
    "    ]).astype(\"uint8\")\n",
    "\n",
    "    # Apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "\n",
    "def auto_gamma(image, target_brightness=0.5):\n",
    "    \"\"\"\n",
    "    Automatically adjusts gamma to normalize image brightness.\n",
    "\n",
    "    Gamma > 1 brightens the image, Gamma < 1 darkens the image.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    brightness = np.mean(gray) / 255.0\n",
    "\n",
    "    # Prevent division by zero or extremely low brightness\n",
    "    if brightness < 1e-3:\n",
    "        gamma = 2.5  # Force strong brightening\n",
    "    else:\n",
    "        gamma = target_brightness / brightness\n",
    "\n",
    "    gamma = np.clip(gamma, 0.3, 3.0)  # Clamp to reasonable range\n",
    "    corrected = gamma_correction(image, gamma)\n",
    "    return corrected, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0850a8b94f141a",
   "metadata": {},
   "source": [
    "# Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dce96073cb5561b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:15:58.234517Z",
     "start_time": "2025-05-12T11:15:58.222368Z"
    }
   },
   "outputs": [],
   "source": [
    "VARIANTS = [\"train\", \"test\", \"val\"]\n",
    "TARGET_RESOLUTION = (1280, 1280)\n",
    "OVERLAP = 0.0 # As a percentage\n",
    "OUTPUT_FOLDER = \"/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiling_output\"\n",
    "INPUT_FOLDER = \"/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/yolo_data\"\n",
    "\n",
    "# Create the output folders\n",
    "for variant in VARIANTS:\n",
    "    if not os.path.exists(os.path.join(OUTPUT_FOLDER, variant)):\n",
    "        os.makedirs(os.path.join(OUTPUT_FOLDER, variant, \"images\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(OUTPUT_FOLDER, variant, \"labels\"), exist_ok=True)\n",
    "\n",
    "files = []\n",
    "for variant in VARIANTS:\n",
    "    for file in os.listdir(os.path.join(INPUT_FOLDER, variant, \"images\")):\n",
    "        files.append(os.path.join(INPUT_FOLDER, variant, \"images\", file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104cb82f83b4bc65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:16:42.667213Z",
     "start_time": "2025-05-12T11:16:00.492355Z"
    }
   },
   "outputs": [],
   "source": [
    "tiles_per_image = {}\n",
    "\n",
    "for file in files:\n",
    "    img_tiles = []\n",
    "    img = cv2.imread(file)\n",
    "\n",
    "    h, w, channels = img.shape\n",
    "\n",
    "    x_tile_step = TARGET_RESOLUTION[0] * (1-OVERLAP)\n",
    "    y_tile_step = TARGET_RESOLUTION[1] * (1-OVERLAP)\n",
    "\n",
    "    number_of_images_w = math.ceil(w / x_tile_step)\n",
    "    number_of_images_h = math.ceil(h / y_tile_step)\n",
    "\n",
    "    for y in range(number_of_images_h):\n",
    "        for x in range(number_of_images_w):\n",
    "            x_min = int(x * x_tile_step)\n",
    "            x_max = int(min(x_min + TARGET_RESOLUTION[0], w))\n",
    "            y_min = int(y * y_tile_step)\n",
    "            y_max = int(min(y_min + TARGET_RESOLUTION[1], h))\n",
    "\n",
    "            tile = img[y_min:y_max, x_min:x_max]\n",
    "            tile_corrected, gamma = auto_gamma(tile)\n",
    "            img_tiles.append({\"w\": w, \"h\": h, \"x_min\": x_min, \"x_max\": x_max, \"y_min\": y_min, \"y_max\": y_max, \"tile\": tile_corrected})\n",
    "\n",
    "    tiles_per_image[file] = img_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e04b317e4bfc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:22:27.093809Z",
     "start_time": "2025-05-12T11:21:07.810849Z"
    }
   },
   "outputs": [],
   "source": [
    "class Label:\n",
    "    \"\"\"Denotes the bounding box in pixels, by having a minimum and maximum x & y.\"\"\"\n",
    "    def __init__(self, x_min, x_max, y_min, y_max):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "\n",
    "\n",
    "def parse_labels(labels: List[str], original_w: int, original_h: int) -> List[Label]:\n",
    "    parsed_labels = []\n",
    "    for label in labels:\n",
    "        coords = label.split(\" \")[1:]\n",
    "\n",
    "        width = float(coords[2])\n",
    "        height = float(coords[3])\n",
    "\n",
    "        x_min = float(coords[0]) - (width/2)\n",
    "        y_min = float(coords[1]) - (height/2)\n",
    "        x_max = x_min + width\n",
    "        y_max = y_min + height\n",
    "\n",
    "        parsed_labels.append(\n",
    "            Label(\n",
    "                x_min * original_w,\n",
    "                x_max * original_w,\n",
    "                y_min * original_h,\n",
    "                y_max * original_h\n",
    "            )\n",
    "        )\n",
    "    return parsed_labels\n",
    "\n",
    "for original_path, tiles in tiles_per_image.items():\n",
    "    label_file = original_path.replace(\"images\", \"labels\")\n",
    "    label_file = \".\".join(label_file.split(\".\")[:-1]) + \".txt\"\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        parsed_labels = parse_labels(f.readlines(), tiles[0][\"w\"], tiles[0][\"h\"])\n",
    "\n",
    "    # Get the variant of the original image\n",
    "    last_folder_name = INPUT_FOLDER.split(\"/\")[-1].split(\"\\\\\")[-1]\n",
    "    search = re.search(rf\"{last_folder_name}(/|\\\\)(.*?)(/|\\\\)images(/|\\\\)\", original_path)\n",
    "    variant = search.group(2)\n",
    "\n",
    "    for i, tile in enumerate(tiles):\n",
    "        # Place tile image\n",
    "        tile_img_file_name = \".\".join(original_path.split(\"/\")[-1].split(\".\")[:-1]) + \"_tile-\" + str(i) + \".png\"\n",
    "        tile_img_path = os.path.join(OUTPUT_FOLDER, variant, \"images\", tile_img_file_name)\n",
    "\n",
    "        cv2.imwrite(tile_img_path, tile[\"tile\"])\n",
    "\n",
    "        # Figure out which labels of the labels of the original image are in the tile, and adapt the coordinates accordingly\n",
    "        tile_labels = []\n",
    "        for label_i, label in enumerate(parsed_labels):\n",
    "            if (tile[\"x_min\"] < label.x_min and tile[\"x_max\"] > label.x_max and\n",
    "                tile[\"y_min\"] < label.y_min and tile[\"y_max\"] > label.y_max):\n",
    "\n",
    "                x_min = label.x_min - tile[\"x_min\"]\n",
    "                y_min = label.y_min - tile[\"y_min\"]\n",
    "                x_max = label.x_max - tile[\"x_min\"]\n",
    "                y_max = label.y_max - tile[\"y_min\"]\n",
    "\n",
    "                tile_width = tile[\"x_max\"] - tile[\"x_min\"]\n",
    "                tile_height = tile[\"y_max\"] - tile[\"y_min\"]\n",
    "\n",
    "                label_relative_width = (x_max - x_min) / tile_width\n",
    "                label_relative_height = (y_max - y_min) / tile_height\n",
    "\n",
    "                tile_labels.append(f\"0 {x_min / tile_width + (label_relative_width / 2)} {y_min / tile_height + (label_relative_height / 2)} \"\n",
    "                                   f\"{label_relative_width} \"\n",
    "                                   f\"{label_relative_height}\")\n",
    "\n",
    "\n",
    "        tile_label_file_name = tile_img_file_name.replace(\".png\", \".txt\")\n",
    "        tile_label_path = os.path.join(OUTPUT_FOLDER, variant, \"labels\", tile_label_file_name)\n",
    "        with open(tile_label_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(tile_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25125f1c9bba1ad1",
   "metadata": {},
   "source": [
    "# Bird generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624b8ad406ae37ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:22:31.152678Z",
     "start_time": "2025-05-12T11:22:31.062846Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageData(BaseModel):\n",
    "    image_name: str\n",
    "    image_paths: List[str] = list()\n",
    "    label_text: str\n",
    "    bird_class: int = None # 0 = crow, 1 = , 2 = , 3 = pigeon, 4 = other\n",
    "    cleaned_file: str = \"\"\n",
    "\n",
    "    def model_post_init(self, context):\n",
    "        self.bird_class = int(self.label_text[0])\n",
    "\n",
    "        return super().model_post_init(context)\n",
    "\n",
    "    def get_random_image_path(self):\n",
    "        random_image = random.choice(self.image_paths)\n",
    "        random_image = self.image_paths[0]\n",
    "\n",
    "        return self.image_name, random_image, self.label_text, self.bird_class\n",
    "\n",
    "    def get_cleaned_image(self):\n",
    "        display(Image(filename=self.cleaned_file))\n",
    "\n",
    "    def get_cleaned_scaled_image(self, new_width, new_height):\n",
    "        img = PILImage.open(self.cleaned_file)\n",
    "        wpercent = (new_width / float(img.size[0]))\n",
    "        hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "        img_resized = img.resize((new_width, hsize), PILImage.Resampling.LANCZOS)\n",
    "        display(img_resized)\n",
    "\n",
    "    def get_cropped_images(self, new_width):\n",
    "        img = PILImage.open(self.cleaned_file)\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        bounding_boxes = self.label_text.split(\"\\n\")\n",
    "        # You only want to take one of the bounding boxes to display because we only want to add one picture into another picture\n",
    "        # So we take the largest one, which has the highest probability to be one that is the most complete bird\n",
    "        sorted_bounding_boxes = sorted(bounding_boxes, reverse=True, key= lambda x: x[3])\n",
    "        for largest_bounding_box in sorted_bounding_boxes:\n",
    "            # when the data is in incorrect format\n",
    "            if len(largest_bounding_box.split(\" \")) != 5:\n",
    "                return False\n",
    "            bird_class, x_center_rel, y_center_rel, width_rel, height_rel = map(float, largest_bounding_box.split(\" \"))\n",
    "            x_center = x_center_rel * img_width\n",
    "            y_center = y_center_rel * img_height\n",
    "            width = width_rel * img_width\n",
    "            height = height_rel * img_height\n",
    "\n",
    "            x_short = x_center - (0.5 * width)\n",
    "            x_long = x_center + (0.5 * width)\n",
    "            y_short = y_center - (0.5 * height)\n",
    "            y_long = y_center + (0.5 * height)\n",
    "            cropped_img = img.crop((x_short, y_short, x_long, y_long))\n",
    "\n",
    "            # now we are scaling the cropped image to the correct size\n",
    "            wpercent = (new_width / float(img_width))\n",
    "            hsize = int((float(img_height) * wpercent))\n",
    "            img_resized = cropped_img.resize((new_width, hsize), PILImage.Resampling.LANCZOS)\n",
    "            if cropped_img.mode != \"RGBA\":\n",
    "                print(\"image is in mode: \", cropped_img.mode, \"converting to RGBA\")\n",
    "                cropped_img = cropped_img.convert(\"RGBA\")\n",
    "            # Extract alpha channel (opacity)\n",
    "            alpha = img_resized.getchannel(\"A\")\n",
    "\n",
    "            # Convert to numpy array for efficient computation\n",
    "            alpha_np = np.array(alpha, dtype=np.float32) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "            # Calculate average opacity\n",
    "            avg_opacity = np.mean(alpha_np)\n",
    "\n",
    "            # Skip image if average opacity is less than 0.05\n",
    "            if avg_opacity < 0.05:\n",
    "                print(\"the opacity is too little for the largest bounding box\")\n",
    "                continue\n",
    "\n",
    "            return img_resized\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfbda08cce877b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:22:35.659814Z",
     "start_time": "2025-05-12T11:22:35.644410Z"
    }
   },
   "outputs": [],
   "source": [
    "class AllImages(BaseModel):\n",
    "    images_dict: Dict[str, ImageData] = dict()\n",
    "\n",
    "    def get_image_list_index(self, bird_classes: Tuple[int] = (0, 1, 2, 3, 4), cleaned_file=False):\n",
    "        \"\"\"gets a list of bird images that satisify the requirement of input\"\"\"\n",
    "        if cleaned_file:\n",
    "            found_image_dict = {index: image_name\n",
    "                        for index, (image_name, image)\n",
    "                          in enumerate(self.images_dict.items())\n",
    "                          if image.bird_class in bird_classes and image.cleaned_file != \"\"}\n",
    "        else:\n",
    "            found_image_dict = {index: image_name\n",
    "                            for index, (image_name, image)\n",
    "                            in enumerate(self.images_dict.items())\n",
    "                            if image.bird_class in bird_classes}\n",
    "        return found_image_dict\n",
    "\n",
    "    def get_random_instance(self, bird_classes: Tuple[int] = (0, 1, 2, 3, 4), cleaned_file=False):\n",
    "        found_image_dict = self.get_image_list_index(bird_classes, cleaned_file)\n",
    "        random_key = random.choice(list(found_image_dict.keys()))\n",
    "        found_image = found_image_dict[random_key]\n",
    "        return self.images_dict[found_image]\n",
    "\n",
    "    def get_random_picture(self, bird_classes: Tuple[int] = (0, 1, 2, 3, 4)):\n",
    "\n",
    "        found_image = self.get_random_instance(bird_classes)\n",
    "        image_name, found_image_path, label_text, bird_class = found_image.get_random_image_path()\n",
    "        display(Image(filename=found_image_path))\n",
    "        return found_image_path\n",
    "\n",
    "    def get_random_clean_image(self, new_width, new_height):\n",
    "        image = self.get_random_instance((0, 3), True)\n",
    "        print(image.image_name)\n",
    "        print(image.cleaned_file)\n",
    "        image.get_cleaned_scaled_image(new_width, new_height)\n",
    "\n",
    "    def get_random_cropped_images(self, new_width):\n",
    "\n",
    "        # Sometimes the cropped image is in the wrong format. So we recursively call this function to retry another one\n",
    "        for i in range(10):\n",
    "            image = self.get_random_instance((0, 3), True)\n",
    "            cropped_image = image.get_cropped_images(new_width)\n",
    "            if cropped_image:\n",
    "                print(\"found image= \", image.image_name)\n",
    "                return cropped_image\n",
    "\n",
    "        print(\"No valid cropped image found after 10 attempts.\")\n",
    "        return None\n",
    "\n",
    "    def get_list_of_paths_crows_pigeons(self):\n",
    "        \"\"\"returns all of the information of the files as a list of lists.\n",
    "        only includes pigeons and crows\"\"\"\n",
    "        found_images_objects = self.get_image_list_index((0, 3))\n",
    "        image_paths = [self.images_dict[image].get_random_image_path() for image in list(found_images_objects.values())]\n",
    "        return image_paths\n",
    "\n",
    "    def copy_crows_pigeons(self, destination_folder: str):\n",
    "        crows_pigeon_paths = self.get_list_of_paths_crows_pigeons()\n",
    "        crows_path = f\"{destination_folder}/crows\"\n",
    "        pigeons_path = f\"{destination_folder}/pigeons\"\n",
    "        if not os.path.exists(destination_folder):\n",
    "            os.mkdir(destination_folder)\n",
    "            os.mkdir(crows_path)\n",
    "            os.mkdir(f\"{crows_path}/labels\")\n",
    "            os.mkdir(pigeons_path)\n",
    "            os.mkdir(f\"{pigeons_path}/labels\")\n",
    "        else:\n",
    "            raise Exception(\"folder already exists\")\n",
    "\n",
    "\n",
    "        for index, (image_name, image_path, label_text, bird_class) in enumerate(crows_pigeon_paths):\n",
    "            bird_cat = \"c\" if bird_class == 0 else \"p\"\n",
    "            image_name = f\"{bird_cat}_{index}\"\n",
    "            if bird_class == 0:\n",
    "                #shutil.copy(image_path, f\"{crows_path}/{image_name}.jpg\")\n",
    "                shutil.copy(image_path, crows_path)\n",
    "                with open(f\"{crows_path}/labels/{image_name}.txt\", \"w\") as f:\n",
    "                    f.write(label_text)\n",
    "\n",
    "            elif bird_class == 3:\n",
    "                #shutil.copy(image_path, f\"{pigeons_path}/{image_name}.jpg\")\n",
    "                shutil.copy(image_path, pigeons_path)\n",
    "\n",
    "                with open(f\"{pigeons_path}/labels/{image_name}.txt\", \"w\") as f:\n",
    "                    f.write(label_text)\n",
    "\n",
    "    def load_removed_background_pictures(self, path: str):\n",
    "        \"\"\"give the folder of where the pictures are that have removed the background\n",
    "        the path folder should contain two folders \"pigeons\" and \"crows\"\n",
    "        \"\"\"\n",
    "        folders = os.listdir(path)\n",
    "        if not (\"pigeons\" in folders and \"crows\" in folders):\n",
    "            raise Exception(\"pigeons and crows doesn't exist in folder\")\n",
    "\n",
    "        for file in os.listdir(f\"{path}/pigeons\"):\n",
    "            if \".DS_Store\" in file:\n",
    "                continue\n",
    "            first_file_name = file.split(\".\")[0]\n",
    "            self.images_dict[first_file_name].cleaned_file = f\"{path}/pigeons/{file}\"\n",
    "\n",
    "        for file in os.listdir(f\"{path}/crows\"):\n",
    "            if \".DS_Store\" in file:\n",
    "                continue\n",
    "            first_file_name = file.split(\".\")[0]\n",
    "            self.images_dict[first_file_name].cleaned_file = f\"{path}/crows/{file}\"\n",
    "\n",
    "    def get_files_in_data_folder(self, path: str):\n",
    "        images_paths = [f\"{path}/images/{file_path}\" for file_path in os.listdir(f\"{path}/images\")]\n",
    "        label_file_names = [file_path for file_path in os.listdir(f\"{path}/labels\")]\n",
    "\n",
    "        for label_file in label_file_names:\n",
    "            if \".DS_Store\" in label_file:\n",
    "                continue\n",
    "            # the same picture has the first part the same but might have had different augmentation\n",
    "            first_file_name = label_file.split(\".\")[0]\n",
    "            if first_file_name in self.images_dict:\n",
    "                image = self.images_dict[first_file_name]\n",
    "            else:\n",
    "                with open(f\"{path}/labels/{label_file}\") as f:\n",
    "                    label_text = f.read()\n",
    "                image = ImageData(image_name=first_file_name,\n",
    "                                  label_text=label_text)\n",
    "            label_file_no_ext = os.path.splitext(label_file)[0]\n",
    "            found_image = [file_name for file_name in images_paths if label_file_no_ext in file_name][0]\n",
    "            image.image_paths.append(found_image)\n",
    "            self.images_dict[first_file_name] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b9bbc8666791045",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T11:22:46.578408Z",
     "start_time": "2025-05-12T11:22:46.513900Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/notebooks/DL---detection-of-birds-in-drone-images/data/Harmful Birds Detection.v1i.yolov11/test/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     all_images.load_removed_background_pictures(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/data/Subject images crows/Subjects not pixelated\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_images\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m all_images_objects = \u001b[43mget_all_images_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_average_bounding_box\u001b[39m(label_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"label_path is the location to the yoloflow.txt file of the image. takes all the bounding boxes of the image and calculate the average\"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mget_all_images_objects\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m all_images = AllImages()\n\u001b[32m      3\u001b[39m repo_path = \u001b[33m\"\u001b[39m\u001b[33m/notebooks/DL---detection-of-birds-in-drone-images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m test_images = \u001b[43mall_images\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_files_in_data_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrepo_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/data/Harmful Birds Detection.v1i.yolov11/test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m train_images = all_images.get_files_in_data_folder(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/data/Harmful Birds Detection.v1i.yolov11/train\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m valid_images = all_images.get_files_in_data_folder(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/data/Harmful Birds Detection.v1i.yolov11/valid\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mAllImages.get_files_in_data_folder\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_files_in_data_folder\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     images_paths = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/images\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m    109\u001b[39m     label_file_names = [file_path \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m os.listdir(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/labels\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m label_file \u001b[38;5;129;01min\u001b[39;00m label_file_names:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '/notebooks/DL---detection-of-birds-in-drone-images/data/Harmful Birds Detection.v1i.yolov11/test/images'"
     ]
    }
   ],
   "source": [
    "def get_all_images_objects():\n",
    "    all_images = AllImages()\n",
    "    repo_path = \"/notebooks/DL---detection-of-birds-in-drone-images\"\n",
    "    all_images.load_removed_background_pictures(f\"{repo_path}/data/Subject images crows/Subjects not pixelated\")\n",
    "    return all_images\n",
    "\n",
    "all_images_objects = get_all_images_objects()\n",
    "\n",
    "\n",
    "def get_average_bounding_box(label_path: str):\n",
    "    \"\"\"label_path is the location to the yoloflow.txt file of the image. takes all the bounding boxes of the image and calculate the average\"\"\"\n",
    "    with open(label_path) as f:\n",
    "        bounding_boxes = f.readlines()\n",
    "\n",
    "    bounding_boxes = [bounding_box.strip().split(\" \") for bounding_box in bounding_boxes]\n",
    "    #bird_class, x_center_rel, y_center_rel, width_rel, height_rel = map(float, largest_bounding_box.split(\" \"))\n",
    "\n",
    "    try:\n",
    "        average_rel_width = sum([float(bounding_box[3]) for bounding_box in bounding_boxes])/len(bounding_boxes)\n",
    "        average_rel_height = sum([float(bounding_box[4]) for bounding_box in bounding_boxes])/len(bounding_boxes)\n",
    "    except ZeroDivisionError:\n",
    "        average_rel_width = 0\n",
    "        average_rel_height = 0\n",
    "    return average_rel_width, average_rel_height\n",
    "\n",
    "\n",
    "def add_picture_to_picture(image_path: str, label_path: str, average_rel_width, average_rel_height):\n",
    "    \"\"\"file_name is the name of the file to be augmented onto\n",
    "    folder_path is where the filename is in\n",
    "    average_rel_width and average_rel_height should come from the function get_average_bounding_box(label_path)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    h, w, channels = img.shape\n",
    "    height_pixels = int(h*average_rel_height)\n",
    "    width_pixels = int(w*average_rel_width)\n",
    "\n",
    "    cropped_pil = all_images_objects.get_random_cropped_images(width_pixels)\n",
    "    if cropped_pil is None:\n",
    "        print(\"cropped image above is None\")\n",
    "        cropped_pil = all_images_objects.get_random_cropped_images(width_pixels)\n",
    "        if cropped_pil is None:\n",
    "            print(\"it is still None, abort\")\n",
    "            return\n",
    "\n",
    "    # Convert PIL to OpenCV format\n",
    "    cropped_np = np.array(cropped_pil.convert(\"RGBA\"))\n",
    "    cropped_cv = cv2.cvtColor(cropped_np, cv2.COLOR_RGBA2BGRA)  # Preserve alpha channel\n",
    "\n",
    "    # Choose a position to paste. At least 3 pixels from the border and half of the picture size to be added\n",
    "    x_offset = random.randint(\n",
    "        int(3 + width_pixels * 0.5),\n",
    "        int(w - 3 - width_pixels * 0.5)\n",
    "    )\n",
    "\n",
    "    y_offset = random.randint(\n",
    "        int(3 + height_pixels * 0.5),\n",
    "        int(h - 3 - height_pixels * 0.5)\n",
    "    )\n",
    "\n",
    "    # Get overlay dimensions\n",
    "    overlay_h, overlay_w = cropped_cv.shape[:2]\n",
    "\n",
    "    # Make sure the overlay fits within the image bounds\n",
    "    if y_offset + overlay_h > h:\n",
    "        overlay_h = h - y_offset\n",
    "        cropped_cv = cropped_cv[:overlay_h, :, :]\n",
    "\n",
    "    if x_offset + overlay_w > w:\n",
    "        overlay_w = w - x_offset\n",
    "        cropped_cv = cropped_cv[:, :overlay_w, :]\n",
    "\n",
    "    # Get the ROI from the original image\n",
    "    roi = img[y_offset:y_offset+overlay_h, x_offset:x_offset+overlay_w]\n",
    "\n",
    "    # Check channels\n",
    "    ch = cropped_cv.shape[2]\n",
    "\n",
    "    # Proper alpha blending\n",
    "    if ch == 4:  # If we have an alpha channel\n",
    "        # Extract the alpha channel and normalize to [0, 1]\n",
    "        alpha = cropped_cv[:, :, 3] / 255.0\n",
    "\n",
    "        # Create a 3-channel alpha mask\n",
    "        alpha_3d = np.dstack((alpha, alpha, alpha))\n",
    "\n",
    "        # Extract BGR channels from overlay\n",
    "        overlay_bgr = cropped_cv[:, :, :3]\n",
    "\n",
    "        # Calculate blended image\n",
    "        blended = (1.0 - alpha_3d) * roi + alpha_3d * overlay_bgr\n",
    "\n",
    "        # Replace the ROI with the blended image\n",
    "        img[y_offset:y_offset+overlay_h, x_offset:x_offset+overlay_w] = blended.astype(np.uint8)\n",
    "    else:\n",
    "        # Just copy if no alpha\n",
    "        img[y_offset:y_offset+overlay_h, x_offset:x_offset+overlay_w] = cropped_cv\n",
    "\n",
    "    # below we are adding the newly generated image that is augmented with one extra bird\n",
    "    # we add the label as well to the dataset\n",
    "    x_offset_rel = x_offset/w\n",
    "    y_offset_rel = y_offset/h\n",
    "    x_rel = overlay_w/w\n",
    "    y_rel = overlay_h/h\n",
    "    new_yolo_label_str = f\"0 {x_offset_rel+0.5*x_rel} {y_offset_rel+0.5*y_rel} {x_rel} {y_rel}\"\n",
    "    with open(label_path, \"a\") as f:\n",
    "        f.write(f\"\\n{new_yolo_label_str}\")\n",
    "    cv2.imwrite(image_path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a1e6fc4aa3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles_per_image is a variable that was created in the image slicing phase\n",
    "for original_path, tiles in tiles_per_image.items():\n",
    "    if \"train\" not in original_path:\n",
    "        continue\n",
    "\n",
    "    label_file = original_path.replace(\"/images\", \"/labels\")\n",
    "    label_file = \".\".join(label_file.split(\".\")[:-1]) + \".txt\"\n",
    "\n",
    "    avg_bb_width, avg_bb_height = get_average_bounding_box(label_file)\n",
    "    if avg_bb_width == 0 or avg_bb_height == 0:\n",
    "        continue\n",
    "\n",
    "    # Here we scale the avg bounding boxes of the original image to a same sized bounding box in a tile/slice.\n",
    "    target_width = avg_bb_width * tiles[0][\"w\"] / TARGET_RESOLUTION[0]\n",
    "    target_height = avg_bb_height * tiles[0][\"h\"] / TARGET_RESOLUTION[1]\n",
    "\n",
    "    for i in range(len(tiles)):\n",
    "        tile_img_file_name = \".\".join(original_path.split(\"\\\\\")[-1].split(\".\")[:-1]) + \"_tile-\" + str(i) + \".png\"\n",
    "        tile_img_path = os.path.join(OUTPUT_FOLDER, \"train\", \"images\", tile_img_file_name)\n",
    "\n",
    "        tile_label_file_name = tile_img_file_name.replace(\".png\", \".txt\")\n",
    "        tile_label_path = os.path.join(OUTPUT_FOLDER, \"train\", \"labels\", tile_label_file_name)\n",
    "\n",
    "        add_picture_to_picture(tile_img_path, tile_label_path, target_width, target_height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867498ff8ac75de",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc11ba5bf21f950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:28:35.650479Z",
     "start_time": "2025-05-12T10:27:58.211810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Running configuration 1/12: lr0=0.001, mosaic=0.0, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 2/12: lr0=0.001, mosaic=0.0, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 3/12: lr0=0.001, mosaic=0.5, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 4/12: lr0=0.001, mosaic=0.5, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 5/12: lr0=0.001, mosaic=1.0, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 6/12: lr0=0.001, mosaic=1.0, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 7/12: lr0=0.005, mosaic=0.0, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 8/12: lr0=0.005, mosaic=0.0, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 9/12: lr0=0.005, mosaic=0.5, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 10/12: lr0=0.005, mosaic=0.5, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 11/12: lr0=0.005, mosaic=1.0, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 12/12: lr0=0.005, mosaic=1.0, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 13/12: lr0=0.01, mosaic=0.0, scale=0.3\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 14/12: lr0=0.01, mosaic=0.0, scale=0.5\n",
      "already process this parameter\n",
      "\n",
      "üîÅ Running configuration 15/12: lr0=0.01, mosaic=0.5, scale=0.3\n",
      "Ultralytics 8.3.131  Python-3.12.0 torch-2.7.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=0.5, multi_scale=False, name=run_15_lr0.01_mos0.5_sc0.3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=grid_search_yolo, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=grid_search_yolo\\run_15_lr0.01_mos0.5_sc0.3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.3, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 821.6107.5 MB/s, size: 2411.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\train\\labels... 1502 images, 1035 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1502/1502 [00:07<00:00, 195.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=1280 at 60.0% CUDA memory utilization.\n",
      "WARNING \u001b[34m\u001b[1mAutoBatch: \u001b[0mintended for CUDA devices, using default batch-size 16\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 1424.3489.4 MB/s, size: 2657.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\train\\labels.cache... 1502 images, 1035 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1502/1502 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 795.2132.0 MB/s, size: 2841.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\val\\labels... 178 images, 129 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:00<00:00, 192.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\val\\images\\DJI_0319_tile-0.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\DL---detection-of-birds-in-drone-images\\data\\tiled_data\\val\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to grid_search_yolo\\run_15_lr0.01_mos0.5_sc0.3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mgrid_search_yolo\\run_15_lr0.01_mos0.5_sc0.3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43myolo_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrid_search_yolo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_lr\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlr0\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_mos\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmosaic\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_sc\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscale\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Get metrics\u001b[39;00m\n\u001b[32m     39\u001b[39m metrics = model.val()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    792\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:212\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:386\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    385\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:300\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcriterion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    298\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.criterion(preds, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:115\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:133\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:154\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    155\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:301\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m y.extend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(torch.cat(y, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:476\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    475\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28mself\u001b[39m.cv1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:79\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Apply convolution, batch normalization and activation to input tensor.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\studie\\Semester 2\\content\\Deep Learning\\group_assignment\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter options\n",
    "learning_rates = [0.001, 0.005, 0.01]\n",
    "mosaic_values = [0.0, 0.5, 1.0]\n",
    "scale_values = [0.3, 0.5]\n",
    "\n",
    "# Prepare CSV to store results\n",
    "results_file = \"grid_search_results.csv\"\n",
    "with open(results_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Run\", \"lr0\", \"mosaic\", \"scale\", \"mAP50\", \"mAP50-95\"])\n",
    "\n",
    "# Run grid search\n",
    "runs = list(product(learning_rates, mosaic_values, scale_values))\n",
    "for i, (lr0, mosaic, scale) in enumerate(runs, start=1):\n",
    "    print(f\"\\nüîÅ Running configuration {i}/12: lr0={lr0}, mosaic={mosaic}, scale={scale}\")\n",
    "\n",
    "    model = YOLO(\"yolov8m.pt\")  # Change to yolov8s.pt or other if needed\n",
    "    if f\"run_{i}_lr{lr0}_mos{mosaic}_sc{scale}\" in os.listdir(\"grid_search_yolo\"):\n",
    "        print(\"already process this parameter\")\n",
    "        continue\n",
    "    # Train\n",
    "    results = model.train(\n",
    "        data=yolo_data,\n",
    "        epochs=50,\n",
    "        imgsz=1280,\n",
    "        batch=-1,\n",
    "        lr0=lr0,\n",
    "        mosaic=mosaic,\n",
    "        scale=scale,\n",
    "        patience=10,\n",
    "        project=\"grid_search_yolo\",\n",
    "        name=f\"run_{i}_lr{lr0}_mos{mosaic}_sc{scale}\",\n",
    "        exist_ok=True,\n",
    "        seed=42,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = model.val()\n",
    "    mAP50 = metrics.box.map50\n",
    "    mAP50_95 = metrics.box.map\n",
    "\n",
    "    # Save results\n",
    "    with open(results_file, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([i, lr0, mosaic, scale, mAP50, mAP50_95])\n",
    "\n",
    "print(\"\\n‚úÖ Grid search complete. Results saved to:\", results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1dbdceda61414",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "The best parameters we found were:\n",
    "- Learning rate:\n",
    "- Mosiac value:\n",
    "- Scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48be39446b0fa526",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T12:12:24.371209Z",
     "start_time": "2025-05-12T12:12:02.912801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.132 üöÄ Python-3.12.10 torch-2.7.0+cu126 CPU (Intel Core(TM) i7-9750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=1280, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=Best model, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=grid_search_yolo, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=grid_search_yolo/Best model, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3388.7¬±1846.7 MB/s, size: 2411.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/train/labels... 1502 images, 1035 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1502/1502 [00:15<00:00, 98.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/train/labels.cache\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=1280 at 60.0% CUDA memory utilization.\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mAutoBatch: \u001b[0mintended for CUDA devices, using default batch-size 16\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3463.9¬±2090.7 MB/s, size: 2657.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/train/labels.cache... 1502 images, 1035 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1502/1502 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 790.9¬±158.9 MB/s, size: 2841.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/val/labels... 178 images, 129 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 178/178 [00:02<00:00, 84.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0m/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/val/images/DJI_0319_tile-0.png: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:789: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to grid_search_yolo/Best model/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mgrid_search_yolo/Best model\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8m.pt\")\n",
    "results = model.train(\n",
    "    data=yolo_data,\n",
    "    epochs=50,\n",
    "    imgsz=1280,\n",
    "    batch=-1,\n",
    "    # lr0=lr0,\n",
    "    # mosaic=mosaic,\n",
    "    # scale=scale,\n",
    "    patience=10,\n",
    "    project=\"grid_search_yolo\",\n",
    "    name=f\"Best model\",\n",
    "    exist_ok=True,\n",
    "    seed=42,\n",
    "    verbose=False,\n",
    ")\n",
    "model.save(\"best_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c3fe8b344d37d",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c2b70798d8d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:174: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "to_infer_folder = \"/home/mathijs/studie/Semester 2/content/deep_learning_group_project/DL---detection-of-birds-in-drone-images/data/tiled_data/test/images\"\n",
    "to_infer = [os.path.join(to_infer_folder, file) for file in os.listdir(to_infer_folder)]\n",
    "\n",
    "model = YOLO(\"best.pt\")\n",
    "results = model(to_infer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a277b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
